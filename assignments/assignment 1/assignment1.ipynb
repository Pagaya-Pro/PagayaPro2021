{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have data from Raleigh, NC, where we recently deployed a flyer program in order to increase public awareness of the possibility of taking a Pagaya loan. You objective is to assess the potency of the flyer program in terms of activation by analyzing production and TU data obtained from the town.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-05T04:06:54.615807Z",
     "iopub.status.busy": "2021-08-05T04:06:54.615496Z",
     "iopub.status.idle": "2021-08-05T04:06:54.620756Z",
     "shell.execute_reply": "2021-08-05T04:06:54.619616Z",
     "shell.execute_reply.started": "2021-08-05T04:06:54.615738Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data from TU "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin with, upload TUs credit information from the relevant quarter (which is 2018Q3). This data is available in our `s3` path under the name `tu_gen_data2018Q3.parquet`. To get to our `s3` path import `DATA_FILES_PATH` from `pagayapro.paths.data_paths`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-05T04:06:54.622020Z",
     "iopub.status.busy": "2021-08-05T04:06:54.621754Z",
     "iopub.status.idle": "2021-08-05T04:06:54.664912Z",
     "shell.execute_reply": "2021-08-05T04:06:54.664000Z",
     "shell.execute_reply.started": "2021-08-05T04:06:54.621994Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pagayapro.paths.data_paths import DATA_FILES_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read the parquet file from `s3`. Only read the columns [\"sequence_num\", \"credit_score\", \"listing_term\", \"int_rate\", \"loan_amnt\", \"monthly_pmt\", \"original_state\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-05T04:06:56.072723Z",
     "iopub.status.busy": "2021-08-05T04:06:56.072449Z",
     "iopub.status.idle": "2021-08-05T04:07:01.509670Z",
     "shell.execute_reply": "2021-08-05T04:07:01.508517Z",
     "shell.execute_reply.started": "2021-08-05T04:06:56.072697Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tu_data= pd.read_parquet(\n",
    "    os.path.join(DATA_FILES_PATH,\"tu_gen_data_2018Q3.parquet\"),\n",
    "                columns = [\"sequence_num\", \"credit_score\", \n",
    "                           \"listing_term\", \"int_rate\" ,\n",
    "                           \"loan_amnt\", \"monthly_pmt\", \n",
    "                           \"original_state\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output the first 5 rows of tu_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your data regarding activation arrives as forms, detailing the loan amount, interest rate offered to the client and whether he or she accepted the offer. These are available on the assignment's `s3` path, which you can import from `pagayapro.path.data_paths` under the name `ASSIGNMENT1_DATA`, in the subdirectory `offers`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-03T03:12:16.748949Z",
     "iopub.status.busy": "2021-10-03T03:12:16.748663Z",
     "iopub.status.idle": "2021-10-03T03:12:16.752843Z",
     "shell.execute_reply": "2021-10-03T03:12:16.751939Z",
     "shell.execute_reply.started": "2021-10-03T03:12:16.748920Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pagayapro.paths.data_paths import ASSIGNMENT1_DATA\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read all files in the subdirectory `offers` of `ASSIGNMENT1_DATA` whose name contains the substring `raleigh`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-04T16:34:28.600581Z",
     "iopub.status.busy": "2021-08-04T16:34:28.600218Z",
     "iopub.status.idle": "2021-08-04T16:34:29.082199Z",
     "shell.execute_reply": "2021-08-04T16:34:29.081138Z",
     "shell.execute_reply.started": "2021-08-04T16:34:28.600553Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "raleigh_forms= (\n",
    "    glob.glob(\n",
    "        os.path.join(ASSIGNMENT1_DATA,\"offers/*raleigh*\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "how many forms are there?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read all the form files and create a table which contains only the last offer given to each borrower. Call this dataframe `raleigh_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raleigh_df= #write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-04T09:43:56.002881Z",
     "iopub.status.busy": "2021-07-04T09:43:56.002602Z",
     "iopub.status.idle": "2021-07-04T09:43:56.037934Z",
     "shell.execute_reply": "2021-07-04T09:43:56.036868Z",
     "shell.execute_reply.started": "2021-07-04T09:43:56.002855Z"
    }
   },
   "source": [
    "How many rows are there in your table? How many distict ids?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can you explain the duplicate ids? Sample 5 ids with multiple lines and display them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop all lines in the dataframe which contain irrelevant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the average interest rate offered to clients? What is the average among clients who accepted the offer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the percentage of activation in this dataframe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enrich your dataframe with additioanl TU features by merging it with the `tu_data` df. Note that the two dataframe share the `id` and `sequence_num` features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the average credit score of the clients? What is the average among clients who accepted the offer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we embark on a statistical test, let's do some validations to make sure that the Raleig sample is not somehow distinct from the general population. Our random sample from the general population is the TU data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a histogram to describe the distribution of requested loan amounts in Raleigh and in the general population. Do the distributions look similar? (A good rule-of-thumb for the number of bins to use is the square-root of the number of unique value.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe the loan amount distribution using `sns.distplot` in the Raleigh dataframe and in TU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to verify that the offered interest rate in Raleigh seem similar to those in the general population. \n",
    "Create a histogram to describe the distribution of offered interest rates in Raleigh and interest rates in the general population. Do the distributions look similar?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe the interest rate distribution using `sns.distplot` in the Raleigh dataframe and in TU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, we want to make sure the credit scores are not visibly different in Raleigh from the general population, as this might mean that the data we obtained from the town might not be representative of the general popuation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a histogram to describe the distribution of credit scores in the Raleigh sample and in the general population. Do the distributions look similar? How many bins did you use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe the credit score distribution using `sns.distplot`  in the Raleigh dataframe and in TU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-04T09:43:56.046655Z",
     "iopub.status.busy": "2021-07-04T09:43:56.046389Z",
     "iopub.status.idle": "2021-07-04T09:43:56.393501Z",
     "shell.execute_reply": "2021-07-04T09:43:56.392695Z",
     "shell.execute_reply.started": "2021-07-04T09:43:56.046630Z"
    }
   },
   "source": [
    "Finally, check whether the loans in both datasets have similar distributions of term (i.e. the amount of months the borrower takes the loan for). What values of term exist in the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLT in action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more thing before we start the test. Our goal is to check whether, for which we'll run some statistical test (you probably already know which) which checks if something belongs to a noraml distribution with some mean and some variance. But... is activation normally distributed in the Raliegh sample? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-04T17:02:18.573264Z",
     "iopub.status.busy": "2021-08-04T17:02:18.572889Z",
     "iopub.status.idle": "2021-08-04T17:02:18.576747Z",
     "shell.execute_reply": "2021-08-04T17:02:18.575947Z",
     "shell.execute_reply.started": "2021-08-04T17:02:18.573237Z"
    }
   },
   "outputs": [],
   "source": [
    "#+2 points if you answer without plotting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason why we can assume normality is because we're checking averages of samples and we're relying on the Central Limit Theorem. We won't prove the theorem, but let's test it for a bit. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function which takes a pandas series S and a natural number n and creates a series of means of samples (with replacement) of n entries from S. Decide for yourself on the length of the output series. You can also use a different data structure if you find it more suitable as the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optionally- also create a function which, given the same input as above, plots a histogram of the series of means, and also indicates the mean of this series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate plots of the series of means of activation in Raleigh and in TU for n = 1, 2, 5, 10, 100, 1000 and 1e6, and indicate their means. Does the distribution approach a normal distribution as $n\\to\\infty$ in your opinion?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that your fuction is ready, it's time to make it part of our infrastucture:\n",
    "* Inside the `pagayapro` repo, create a branch called `feature/{your_user_name}/clt_tester/`.\n",
    "* Create a file called `clt.py` inside `~/pagayapro/users/{user_name}/assignment1/` \n",
    "* Copy the two finctions you just created into the file.\n",
    "* Push your file to `main`, open a PR and ask your mentor to review it. Once approved, rebase and merge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical test "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned in the Notion doc, the activation in Raleigh before deployment was measured at 6.85%. In this section, you will run a test to check whether the activation in Raleigh is indeed different following the flyer program. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-22T03:31:05.431001Z",
     "iopub.status.busy": "2021-08-22T03:31:05.430647Z",
     "iopub.status.idle": "2021-08-22T03:31:05.444551Z",
     "shell.execute_reply": "2021-08-22T03:31:05.443762Z",
     "shell.execute_reply.started": "2021-08-22T03:31:05.430918Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from statsmodels.stats.proportion import proportions_ztest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run a statistical test to check whether the current activation rate in Raleigh is different from 6,85%:\n",
    "1. What are your hypotheses?\n",
    "2. Setting your confidence level at 95%, what is the probability of an error of type I in your test? What is the probability of a type II error? What is the power of your test?\n",
    "3. Is the sample mean significantly different from the past value of 6.85?\n",
    "4. Interpret your results. Can you declare the program as successful in increasing activation (what is the probability of wrongfully declaring it successful)?\n",
    "5. Are there any other possible explanations for the change in activation that you observed? Discuss the possible drawbacks of our current test.\n",
    "\n",
    "Use `proportions_ztestz` from `statsmodels.stats.proportion` in order to perform the test. Are your results significant?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you see any possible problems in your methodology? What other factors can explain the change in activation, if such a change exists?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# A/b test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to isolate temporal and environmental effects on activation, we'll run an a/b test, comparing the activation rate in Raleigh to that of Lubbock, TX, another town in which Pagaya operates without a flyer program. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The activation forms for Lubbock also appear in the `offers` subdirectory of the `ASSIGNMENT1_DATA` folder. Read all files from this folder whose name contains the substring 'lubbock', and create a dataframe of final offers called `lubbock_df`, drop unnecessary rows from it, and enrich it with TU features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the average interest rate in Lubbock? What is it among accepting borrowers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the average credit score in Lubbock? What is it among accepting borrowers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the rate of activation in Lubbock?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caution about a/b test "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this is not a classical a/b test. Usually, in such a test, one creates two random subgroups from the general population and applies a manipulation to one of them in order to see if the manipulation had an effect. However, in this case, it is very complicated to isolate a random subgroup from any population and deploy the flyer program only on it.\n",
    "Thus, due to production limitation, we have to make do with what we got."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to justify the use of an a/b test for this situation, we should at least verify that the borrower population in Lubbock and Raleigh seem as though they were randomly selected from the general population. We have actually already checked it for Raleigh above. Add TU features to the loan amount, interest rate, credit score and term comparisons and see if there any major differences between the three populations (Lubbock, Raleigh and TU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing the test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-07-01T03:36:51.530342Z",
     "iopub.status.idle": "2021-07-01T03:36:51.530744Z"
    }
   },
   "source": [
    "Use the function you created in the file `clt.py` to plot the series of means of activation in Raleigh and Lubbock for samples of size n= 1, 10, 1000 and 1e6, and compare the means of the two histograms. To they seem to converge to the same value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply a statistical test to see whether the activation rate in Raleigh, NC is significantly larger than Lubbock, TX. Clearly state and validate your test's assumption. \n",
    "\n",
    "1. What are your hypotheses?\n",
    "2. Setting your confidence level at 95%, what is the probability of an error of type I in your test? What is the probability of a type II error?\n",
    "3. Are the two samples significantly different? \n",
    "4. Interpret your results. Can you declare the program as successful in increasing activation? What is the probability of wrongfully declaring it successful?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `proportions_ztestz` function from `statsmodels.stats.proportion` to perform a comparison between the activation probability in Lubbock and in Raleigh. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-07-01T03:36:51.542043Z",
     "iopub.status.idle": "2021-07-01T03:36:51.542428Z"
    }
   },
   "source": [
    "Interprest your results; do you observe a significant change in activation in Raleigh? What is the probability of wrongfully declaring the program successful?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing for activation in sub-populations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you applied the a/b test to the Lubbock and Raleigh datasets, it is now required that you make a similar comparison on different subsets of the data within various credit score and loan amount bins. To do so, let us first decide on how to segment our data.\n",
    "\n",
    "You are asked to divide your data into 10 bins according to loan amount, and then divide each of those bins into 10 bins according to loan amount. The most staightforward way of doing this is by using the `pd.qcut` function; first dividing the data into 10 quantiles of credit score and then dividing each of these quantiles into 10 according to loan amount."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide your datasets as above and add two columns to your data which indicate the (credit_score, loan_amnt) of the bin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run an a/b test on each of the bins in your data and create a $10\\times 10$ table containing the $p$ value of each test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the [seaborn.heatmap](https://seaborn.pydata.org/generated/seaborn.heatmap.html) function to represent your table as a heatmap. You can also use it on a suitable table of boolean values to get a more robus representation of which experiments turned out significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the probability of falsely rejecting the null hypothesis in each cell? What is the probability of falsely rejecting the null hypothesis in at least one of the cells (i.e. what is the **family wise error rate**)?\n",
    "\n",
    "In other words, given that we made $m$ experiments, with confidence level $\\alpha$, what is the probabilty $\\mathbb{P}\\left(\\bigcup_i X_i \\le q_{\\alpha}\\right)$? \n",
    "\n",
    "_Note_: you may assume that sampling from different bins are independent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonferroni correction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way of assuring that your FWER remains below $\\alpha=0.05$ is to apply a <a href=\"https://en.wikipedia.org/wiki/Bonferroni_correction\">**Bonferroni correction**</a>. This means that we set the signficance level of each individual test to $\\alpha/m$, where $m=100$ is the number of tests (see, e.g., Wikipedia for an explanation). \n",
    "\n",
    "Use a Bonferroni correction on your segmented a/b test and output a $10\\times10$ table of boolean values, indicating which test gave a significant result under this correction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many of your tests came out significant under this correction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize your result using the [seaborn.heatmap]() function and compare to the previous heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BH correction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Bonferroni correction is often criticized for being too restrictive, as you probably noticed in the last subsection. A more commonly used method for correcting $\\alpha$ values is the Benjamini-Hochberg method. You can <a href=\"https://www.youtube.com/watch?v=K8LQSvtjcEo\">check out the quest</a> for a nice explanation about it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by arranging the $p$-values of your original test as a series and listing them in ascending order. You can use the `pd.DataFrame.stack` and `pd.Series.sort_values` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot your sorted $p$-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each entry of the new series, compute the value compute the value $\\frac{i}{m}\\cdot\\alpha$, where $i$ is the ordinal place of the entry (i.e. 1,2,...len(series)), $m=100$ and $\\alpha=0.05$ as above. Add a plot of these values to your $p$-values plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the largest index $i$ for which the $p$-value in the $i$-th place in the series is smaller than $\\frac{i}{m}\\cdot\\alpha$. That is- the last index in the plot you just created where the $p$-values line lies below the _adjusted_-$\\alpha$ line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reject the null hypothesis for all test before and including this index, and accept it for all others. Reformat your table into a $10\\times 10$ dataframe; for which bins did we get a significant result in the corrected test?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize your result using the [seaborn.heatmap]() function and compare to the previous heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validating your results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can validate your results (for both corrections) using the <a href= \"https://www.statsmodels.org/dev/generated/statsmodels.stats.multitest.multipletests.html\">`statsmodels.stats.multitest.multipletests`</a> package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}