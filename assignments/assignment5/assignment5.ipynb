{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Assignment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, you will train and tune a model for predicting the IRR of indiviual loans from 2017Q1--Q2 issued by the Prosper platform. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import numpy_financial as npf\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pagayapro.paths.data_paths import ASSIGNMENT5_DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_parquet(os.path.join(ASSIGNMENT5_DATA, \"prosper_data.parquet\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the first five rows of your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many rows are there in your data? How many columns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the cashflows from the follwing path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cashflows = pd.read_parquet(os.path.join(ASSIGNMENT5_DATA, \"prosper_cashflows.parquet\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the first five cashflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the shape of the cashflows dataframe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify matching ID sets and make sure they are in the same order\n",
    "We will use the cashflows table in order to generate labels for our prediction model. To do so, we must verify that the two data sets describe the same loans and in the same order. Please do so now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-25T07:37:28.285966Z",
     "iopub.status.busy": "2021-08-25T07:37:28.285500Z",
     "iopub.status.idle": "2021-08-25T07:37:28.290076Z",
     "shell.execute_reply": "2021-08-25T07:37:28.288923Z",
     "shell.execute_reply.started": "2021-08-25T07:37:28.285935Z"
    }
   },
   "source": [
    "## Compute IRR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the IRR each of the rows in the cashflows table individually, and convert them to annual IRR in percents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for NaNs in the target: How many NaNs are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine why the NaNs in the cashflows appear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fix the NaN according to your findings. Make sure there are no more NaNs in the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing for model train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the data and see if there are features which we can't/shouldn't use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the different dtypes? how many from each dtype?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important: We must remove the co_mob, co_amount features from the data since they are not features we can use when deciding whether to approve a loan.\n",
    "These features tell us how the payments of the loan behaved, which is information from the future which we don't have at the time of the approval decision.\n",
    "\n",
    "However, we will not remove them yet, as we will use them later on to make sure we have the same ratio of charged-off loans in our train and test datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-17T15:21:49.381464Z",
     "iopub.status.busy": "2021-08-17T15:21:49.380818Z",
     "iopub.status.idle": "2021-08-17T15:21:49.391966Z",
     "shell.execute_reply": "2021-08-17T15:21:49.390656Z",
     "shell.execute_reply.started": "2021-08-17T15:21:49.381253Z"
    }
   },
   "source": [
    "Lets look specifically at the non-numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is safe to assume the date is not correlated to the IRR (and even if, for some reason, it is, there is no way for our model to correctly evaluate loans with future dates which are not used in training). Therefore, we can drop the date feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the remaining categorical features. How many distinct values appear in each of these features?\n",
    "\n",
    "Decide how to manipulate them so they can be used in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove features with high NaN rate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check which features have high NaN values and decide how to treat them. Which is the feature with the most NaN values?\n",
    "\n",
    "Remember that the XGBoost model can handle NaN values in the features, but the more NaNs we have, the harder it will be for the model to learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking validity of cashflows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the payments of each loan for every month the loan existed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find something in this data that doesn't make sense. Figure out why/when this happens. How many times does this behavior happens? Determine how to treat this behavior. If you decide to drop these rows- make sure you do not create a CO-bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to make sure that we do not creat a CO balance by dropping a subset of loans, we should verify that the percentage of CO loans is roughly the same before and after dropping these loans."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-25T11:31:40.329247Z",
     "iopub.status.busy": "2021-08-25T11:31:40.328847Z",
     "iopub.status.idle": "2021-08-25T11:31:40.337295Z",
     "shell.execute_reply": "2021-08-25T11:31:40.335945Z",
     "shell.execute_reply.started": "2021-08-25T11:31:40.329216Z"
    }
   },
   "source": [
    "Compute the CO rate of full data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-25T11:32:20.698406Z",
     "iopub.status.busy": "2021-08-25T11:32:20.697980Z",
     "iopub.status.idle": "2021-08-25T11:32:20.706378Z",
     "shell.execute_reply": "2021-08-25T11:32:20.705034Z",
     "shell.execute_reply.started": "2021-08-25T11:32:20.698375Z"
    }
   },
   "source": [
    "Compute the CO rate of data without invalid payments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note_: Since the difference is very small, it is safe to say we are not creating a CO bias and we can safely remove the entries. \n",
    "We note that though we have determined that we are not creating a CO-bias by removing these entries, it is still very much possible that we are creating some other bias. Since the number of samples is really quite small, it is ok to ignore that for the moment (but better keep that in mind in case the model fails or vastly underperforms for some reason)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Train test/validation split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As stated before, to make sure our model is well trained and tested we want to make sure we have roughly the same CO rate in the train and test sets. To do that, we use the \"stratify\" argument in the train_test_split function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T08:18:34.730365Z",
     "iopub.status.busy": "2021-09-27T08:18:34.729989Z",
     "iopub.status.idle": "2021-09-27T08:18:34.737410Z",
     "shell.execute_reply": "2021-09-27T08:18:34.736312Z",
     "shell.execute_reply.started": "2021-09-27T08:18:34.730337Z"
    }
   },
   "source": [
    "Read about using the \"stratify\" argument in [train-test-splitting](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html). Use \"stratify\" and make sure you have the same CO-rate in train and test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This argument will recieve a boolean column stating whether the loan was a CO. Lets create that column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now split the data to train and test using the sklearn function - `train_test_split`. Make sure the train and test have roughly the same CO rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the CO ratio is indeed very similar between the datasets.\n",
    "Now lets drop the CO feature from the data to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running XGB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import the XGBoost package and create an empty dictionary for saving the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "evals_result = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we set the train and test matrices for the XGBoost (see [here](https://xgboost.readthedocs.io/en/latest/get_started.html)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(X_train, y_train)\n",
    "dtest = xgb.DMatrix(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of how to run XGBoost (the following hyperparameters are completely arbitrary).\n",
    "\n",
    "Notice that the `num_boost_round` hyperparameter must by specified in the xgb.train function, and not as part of the params dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"booster\": \"gbtree\",\n",
    "    \"max_depth\": 6,\n",
    "    \"colsample_bytree\": 1,\n",
    "    \"subsample\": 1,\n",
    "    \"eta\": 0.3,\n",
    "    \"gamma\": 1,\n",
    "    \"min_child_weight\": 1,\n",
    "    \"max_delta_step\": 0,\n",
    "    \"nthread\": 8,\n",
    "    \"seed\": 42,\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    \"eval_metric\": \"rmse\",\n",
    "}\n",
    "num_boost_round = 20\n",
    "\n",
    "model = xgb.train(params, dtrain,  evals=[(dtest, 'eval'), (dtrain, 'train')], evals_result=evals_result, num_boost_round=num_boost_round)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose an initial setting of hyperparameters on which you will later perform a hyperparameter grid search using cross validation. A good place to start the search from is important in order to establish good results (we need to search in the right area). Do not change the loss function. Fill in your initial values in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore the content of the `evals_result` dictionary, and find the values of the loss function for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the loss curves of the training and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that training for longer (more rounds) might improve the training set RMSE but will achieve a higher RMSE on the test set. This happens due to overfitting on the train set. You can verify this phenomenon by setting the `num_boost_round` parameter to 100."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters tuning using cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will use cross-validation in order to fine-tune the hyperparameters.\n",
    "Cross-validation is a good way to evaluate our model with as little bias as possible from the training/test set we created. This is beacuse cross validation uses k-folds, in which, each fold is a different choice of test (validation) set, eventually using the entire data inserted as both train and test set.  \n",
    "\n",
    "In order to avoid overfitting the hyperparameters, we will insert only the training set to the cross-validation, thereby creating 3 datasets for each fold; the original test set, the per-fold training set, and the per-fold test set (more accurately called - validation set, which is a test set used during training but not for the final evaluation of the model)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting parameters for grid-search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the gridsearch hyperparameters by indicating the values checked for each parameter. Make sure not to use too many values since the number of permutations increases multiplicatively!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gridsearch_params = [\n",
    "    (max_depth, min_child_weight, colsample_bytree, subsample, eta, gamma)\n",
    "    for max_depth in #set values here\n",
    "    for min_child_weight in #set values here\n",
    "    for colsample_bytree in #set values here\n",
    "    for subsample in #set values here\n",
    "    for eta in #set values here\n",
    "    for gamma in #set values here\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run k-fold cross validation on all permutations of parameters in the gridsearch. Save the hyperparameters of the best permutation.\n",
    "Make sure this run doesn't take more than 1 hour. The CV step can be done using [XGBoost's cv method](https://www.rdocumentation.org/packages/xgboost/versions/1.4.1.1/topics/xgb.cv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training using  the best hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets now load the best hyperparameters and train the model. Then we will finally test the model on our test set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "evals_result = dict()\n",
    "\n",
    "params = {\n",
    "    \"booster\": \"gbtree\",\n",
    "    \"max_depth\": #set values here,\n",
    "    \"colsample_bytree\": #set values here,\n",
    "    \"subsample\": #set values here,\n",
    "    \"eta\": #set values here,\n",
    "    \"gamma\": #set values here,\n",
    "    \"min_child_weight\": #set values here,\n",
    "    \"max_delta_step\": 0,\n",
    "    \"nthread\": 8,\n",
    "    \"seed\": 42,\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    \"eval_metric\": \"rmse\",\n",
    "}\n",
    "\n",
    "model = xgb.train(params, dtrain,  evals=[(dtest, 'eval'), (dtrain, 'train')], evals_result=evals_result, num_boost_round=num_boost_round)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the final test and train RMSE you got?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intuitively, an RMSE of ~35, when the possible values of the target are (-100, 35), is not good at all.\n",
    "As a sanity check, lets look at the feature importance in our model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run feature importance for you model. What are the 3 most important features in the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ax = xgb.plot_importance(model)\n",
    "fig = ax.figure\n",
    "fig.set_size_inches(24, 16)\n",
    "ax.tick_params(axis='both', which='major', labelsize=20)\n",
    "plt.xlabel('F score', fontsize=24)\n",
    "plt.ylabel('Features', fontsize=24)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-02T14:25:11.823474Z",
     "iopub.status.busy": "2021-09-02T14:25:11.823095Z",
     "iopub.status.idle": "2021-09-02T14:25:11.832024Z",
     "shell.execute_reply": "2021-09-02T14:25:11.830995Z",
     "shell.execute_reply.started": "2021-09-02T14:25:11.823444Z"
    }
   },
   "source": [
    "The above graph describes the features which had the greatest impact on our model. We can use this data as a sanity check to verify our model.\n",
    "For example, the fact that int_rate is the most important feature is very encouraging since if the loan did not CO, the IRR is supposed to be very close to the interest rate.  \n",
    "\n",
    "We can also see that the 'debt_to_income' and 'loan_amount' features are at the top as well which is another good sign for the performance of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check what is the RMSE of the test set if instead of our model we use the mean IRR of the train set. What is the difference between this result and our model's result? What does that mean about our model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazingly, the RMSE when simply taking the mean IRR of the train set is only slightly higher than our model. Usually this means that our model is terrible, however, in this case it is simply very hard to create a model which predicts the IRR of a specific loan (you are more than welcome to try and do better!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furtunately, this is not the requirement of the assignment. We only need to identify the best loans in a given volume to get the highest IRR possible. Lets see how our model fairs in this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking the performance of the model on a portfolio of loans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a scatter plot for the predicted IRR vs the true IRR of the loans in the test set. Can you detect any visible trend? Try to use different stratifications of the data to further investigate the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting volume-return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the performance of your model (IRR-wise) if it had to choose the best X-volume loans (similar to what we did in the previous assignment), for X being various possible volumes of subsets of the overall data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the IRR if we set a volume of 1/5 out of the total volume? How would you evaluate your model in this task?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the IRR of the total test set is 4, a result of 7.4% is very good! Hurray!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see in the graph above, the first buckets which sum up to 10MM$ have a very high IRR (>10% as apposed to ~4% of the total portfolio). Furthermore, we see that the more loans we take, the smaller our IRR gets (most of the time).\n",
    "This means that bins with lower predicted IRR actually do have a lower IRR, as a portfolio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-25T14:57:20.243638Z",
     "iopub.status.busy": "2021-08-25T14:57:20.243245Z",
     "iopub.status.idle": "2021-08-25T14:57:20.251939Z",
     "shell.execute_reply": "2021-08-25T14:57:20.250660Z",
     "shell.execute_reply.started": "2021-08-25T14:57:20.243608Z"
    }
   },
   "source": [
    "From this we can learn that though it seemed that our model did not learn well to predict the IRR of a single loan, it did do a good job when choosing the best loans.\n",
    "This is in contrast to constantly predicting the mean IRR, which fairs similarly to our model when predicting a single loan, but will fair poorly (randomly) when choosing the best X loans."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also compare this model to the model created in the previous assignment and see this model better predicts the best loans in a given volume. Import your model and plot your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, as a comparison to the industry standard- plot the volume-return attained by selecting the top N loans in terms of credit score to get a given volume. How does your mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
