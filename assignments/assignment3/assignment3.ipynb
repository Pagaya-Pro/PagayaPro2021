{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# House Pricing - A Statistical Analysis & Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will work on this step by step.\n",
    "Topics:\n",
    "1. Data Preparation\n",
    "2. Data Analysis\n",
    "3. Statistical Tests\n",
    "4. Implement a linear regression model\n",
    "5. Exploring model results\n",
    "6. Some visualzations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ceecb894-96f2-4208-b7a1-2383b8e9ac9d",
    "_uuid": "dbb42ffc-c5e0-45bf-9d32-fb17433756ff"
   },
   "source": [
    "# Importing necessary libraries and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "65d8108d-0e85-46f7-89f1-03c5b17759b1",
    "_kg_hide-input": true,
    "_uuid": "14ac2156-60b9-4e98-9423-9b428f7dbc3c",
    "execution": {
     "iopub.execute_input": "2021-10-18T03:13:00.197053Z",
     "iopub.status.busy": "2021-10-18T03:13:00.196750Z",
     "iopub.status.idle": "2021-10-18T03:13:00.201366Z",
     "shell.execute_reply": "2021-10-18T03:13:00.200358Z",
     "shell.execute_reply.started": "2021-10-18T03:13:00.197025Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-18T03:13:00.796598Z",
     "iopub.status.busy": "2021-10-18T03:13:00.796336Z",
     "iopub.status.idle": "2021-10-18T03:13:00.802502Z",
     "shell.execute_reply": "2021-10-18T03:13:00.801709Z",
     "shell.execute_reply.started": "2021-10-18T03:13:00.796570Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-18T03:13:01.429156Z",
     "iopub.status.busy": "2021-10-18T03:13:01.428901Z",
     "iopub.status.idle": "2021-10-18T03:13:01.434408Z",
     "shell.execute_reply": "2021-10-18T03:13:01.433680Z",
     "shell.execute_reply.started": "2021-10-18T03:13:01.429129Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.style.use('seaborn')\n",
    "sns.set(palette='bright')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.options.display.float_format = '{:,.4f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pagayapro.paths.data_paths import ASSIGNMENT3_DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f4ed3d7b-d178-490f-873b-6529477ca5b1",
    "_uuid": "6aa76191-8742-4be2-a02f-6a64ae3dab29"
   },
   "source": [
    "# Load the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "fffa6add-e9e7-4e9a-9323-7d25e676f3ce",
    "_kg_hide-input": true,
    "_kg_hide-output": false,
    "_uuid": "8bce0dd4-7e12-4545-b790-5d319a239412",
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(os.path.join(ASSIGNMENT3_DATA, \"house_features.csv\"))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-30T11:03:31.229025Z",
     "iopub.status.busy": "2021-06-30T11:03:31.228650Z",
     "iopub.status.idle": "2021-06-30T11:03:31.232924Z",
     "shell.execute_reply": "2021-06-30T11:03:31.231956Z",
     "shell.execute_reply.started": "2021-06-30T11:03:31.228998Z"
    }
   },
   "source": [
    "## Split to train and test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will split the data to train and test.\n",
    "<br>We can't perform statistical analysis which includes the test data, since that would lead to **overfitting** on the test set (we will elaborate on that later on). Therfore we will first split the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-08-19T02:14:30.763097Z",
     "iopub.status.idle": "2021-08-19T02:14:30.763396Z",
     "shell.execute_reply": "2021-08-19T02:14:30.763251Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = data.sample(frac=0.8,random_state=1) # random_state is the random_seed\n",
    "test = data.drop(train.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ed598341-4a28-4971-bb3a-cc8bdffa06f0",
    "_uuid": "6eb72da0-da8a-45d3-be95-7accd87a5a89"
   },
   "source": [
    "# Explore the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-17T14:01:50.939643Z",
     "iopub.status.busy": "2021-06-17T14:01:50.939280Z",
     "iopub.status.idle": "2021-06-17T14:01:50.943788Z",
     "shell.execute_reply": "2021-06-17T14:01:50.942689Z",
     "shell.execute_reply.started": "2021-06-17T14:01:50.939616Z"
    }
   },
   "source": [
    "## Number of rows and columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find how many rows and columns we have in train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6b9a990b-648b-4e81-a312-cc29ae7ff11a",
    "_kg_hide-input": true,
    "_uuid": "9df6eafa-024c-4b9b-b065-4cea0185017a",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-17T13:45:43.404502Z",
     "iopub.status.busy": "2021-06-17T13:45:43.403081Z",
     "iopub.status.idle": "2021-06-17T13:45:43.443341Z",
     "shell.execute_reply": "2021-06-17T13:45:43.438752Z",
     "shell.execute_reply.started": "2021-06-17T13:45:43.404432Z"
    }
   },
   "source": [
    "## Present a statistical description of the numerical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write down the mean, std, quantiles, etc of each of these features (can be done in one line of code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General questions on the data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-17T12:14:11.821784Z",
     "iopub.status.busy": "2021-06-17T12:14:11.820986Z",
     "iopub.status.idle": "2021-06-17T12:14:11.833755Z",
     "shell.execute_reply": "2021-06-17T12:14:11.826836Z",
     "shell.execute_reply.started": "2021-06-17T12:14:11.821738Z"
    }
   },
   "source": [
    "How many features are numerical? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the median of the LotFrontage feature?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the feature with the largest standard deviation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is data the type of each feature?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "98dd44da-454a-405e-aa49-9ae81e7da40b",
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "cec8da66-8480-4b5f-80ef-bce32f5f7130",
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9eef0002-26bb-41f9-bc18-07832d848410",
    "_uuid": "e908bc68-dec3-46b8-a021-7c03d5a08a7b"
   },
   "source": [
    "As you can see there are multiple types of features.   \n",
    "Most of the features are object type (includes string values in the variable)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression assumes a linear relation between every feature and the target variable.\n",
    "First, we want to calculate the Pearson Correlation: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the correlation of all the features with the target variable (i.e. SalePrice) in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1aeb267-3868-470d-87bc-1d3d02832dd9",
    "_kg_hide-input": true,
    "_uuid": "02155550-8856-4aad-9f40-a757774a7ab2",
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort the features according to the absolute value of their correlation with SalePrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b06bd2d5-6523-42b4-8db6-cac4df7e8ef2",
    "_uuid": "a5ead127-432f-4c17-8d90-8e1bad62f249"
   },
   "source": [
    "So we see that **'OverallQual'** is the most correlated feature. Let's put it in a scatter plot with the target variable and see how it looks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "dad900e1-1b35-4e37-9c4b-aa605db90430",
    "_uuid": "6303094e-aa22-4cb2-a34e-7c08e1043bad"
   },
   "source": [
    "**SalePrice vs OverallQual**\n",
    "\n",
    "Create a scatterplot od SalePrice against OverallQual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.status.busy": "2021-08-19T02:14:30.764446Z",
     "iopub.status.idle": "2021-08-19T02:14:30.764753Z",
     "shell.execute_reply": "2021-08-19T02:14:30.764604Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_scatterplot(y, x):\n",
    "    plt.subplots(figsize = (12,8))\n",
    "    sns.scatterplot(y = y, x = x);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optional** use the function above to create your scatter plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "2e5dae19-f6e8-4bef-b080-a1427fc7b1b9",
    "_uuid": "96a8e53a-5c18-4f38-a7f5-e2e0f3f31ebc"
   },
   "source": [
    "Generally, a scatter plot is not the best way to visualize categorical variables. However; **OverallQual** is an **ordinal** variable, which means that its categories have a specific order (for example 8 is better than 7), so a scatter plot does make sense in this case. In the graph above we can see that there is an apparent relationship between the two features. The price of the houses increases with the overall quality. Let's examine a few more features to determine the outliers. Let's focus on the numerical variables this time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5ba8e501-0e12-4cfe-b40f-8830232b5ff3",
    "_uuid": "be004cfe-4ca7-44b6-a960-45c32ff9e4d5"
   },
   "source": [
    "**SalePrice vs GrLivArea**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create a scatter plot of SalePrice vs GrLivArea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "2136bd32-dfc0-4003-bc43-d96a8bb86278",
    "_uuid": "1cc96334-adcd-4f8b-85f5-96c6e6089413"
   },
   "source": [
    "As you can see, there are two outliers in the plot above. We will get rid off them later (and discuss why we should do this???). Let's look at another scatter plot with a different feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "0a28ed58-55f9-42a7-9d0f-d2d5198240dd",
    "_uuid": "b30a1bbb-6a8a-429b-872b-9f110025d33d"
   },
   "source": [
    "**SalePrice vs TotalBsmtSF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ab918ad4-77b1-49fb-9211-9e4406e67fa3",
    "_kg_hide-input": true,
    "_uuid": "6ff7678f-ac40-456f-b074-8baafd41c21a",
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9c0e0694-d33e-4502-b73a-d35d50f7df80",
    "_uuid": "2ca678ef-1c15-4e83-89da-9c1c2b24c2a3"
   },
   "source": [
    "**SalePrice vs 1stFlrSF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "27a1ab1e-9741-4e24-b333-8d6354fcf142",
    "_kg_hide-input": true,
    "_uuid": "d754a68a-1397-4df5-91e7-2ae3e5d41a89",
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What have we discovered so far?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6cc563ea-81cc-4b23-b1a2-1922af7d4cd2",
    "_uuid": "23f417ba-d0e5-40bb-94c9-db9f12e4fbab"
   },
   "source": [
    "* Our target variable shows an unequal level of variance across most features. This is called **Heteroscedasticity (will be elaborated later)** and is a red flag for the multiple linear regression model.\n",
    "* There are some outliers in the scatter plots.\n",
    "\n",
    "<br>Let's deal with these outliers by removing them from the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4f4c4f41-83bc-42df-86a4-eaa0e01d5f20",
    "_kg_hide-input": true,
    "_uuid": "664e9a98-b288-486b-9ca8-e58820a4440c",
    "execution": {
     "iopub.execute_input": "2021-07-20T10:09:17.191547Z",
     "iopub.status.busy": "2021-07-20T10:09:17.191277Z",
     "iopub.status.idle": "2021-07-20T10:09:17.198361Z",
     "shell.execute_reply": "2021-07-20T10:09:17.197458Z",
     "shell.execute_reply.started": "2021-07-20T10:09:17.191521Z"
    },
    "tags": []
   },
   "source": [
    "Remove the two outliers we found in the SalePrice-GrLivArea plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6c054c3e-12fd-4775-a1ae-934e7e7efd33",
    "_uuid": "2588dd58-d4f0-420a-a7c9-b2e644001904"
   },
   "source": [
    "# Assumptions of Regression\n",
    "\n",
    "* **Linearity** \n",
    "* **Homoscedasticity (Constant Error Variance vs Heteroscedasticity).**\n",
    "* **The target variable is normally distributed when the features values are fixed.**\n",
    "* **Multivariate Normality (Normality of Errors).**\n",
    "* **No or little Multicollinearity.** \n",
    "\n",
    "Since we fit a linear model, we assume that the relationship between the features and the target is **linear**, and the errors, or residuals, are pure random fluctuations around the true line.    \n",
    "We expect that the variability in the dependent variable doesn't increase as the value of the features increases, which is the assumptions of equal variance, also known as **Homoscedasticity**.  \n",
    "We also assume that the observations are independent of one another (**No Multicollinearity**), and that there is no correlation between sequential observations.\n",
    "\n",
    "These assumptions often happen simultaneously. In other words, if we see that one of these assumptions holds in the dataset, it's more likely that other assumptions mentioned above will hold as well.  \n",
    "Therefore, we can find and fix various assumptions with a few unique techniques.\n",
    "\n",
    "So, **How do we check regression assumptions? We fit a regression line and look for the variability of the response data along the regression line.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linearity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression requires the relationship between each independent variable and the dependent variable to be linear.  \n",
    "The linearity assumption can be tested with scatter plots.  \n",
    "The following example depicts a case where little linearity is present. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-17T14:16:32.271290Z",
     "iopub.status.busy": "2021-06-17T14:16:32.270925Z",
     "iopub.status.idle": "2021-06-17T14:16:32.287128Z",
     "shell.execute_reply": "2021-06-17T14:16:32.286037Z",
     "shell.execute_reply.started": "2021-06-17T14:16:32.271264Z"
    }
   },
   "source": [
    "Scatter plotting for SalePrice and GrLivArea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-08-19T02:14:30.765718Z",
     "iopub.status.idle": "2021-08-19T02:14:30.766051Z",
     "shell.execute_reply": "2021-08-19T02:14:30.765904Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize = (12,8), ncols=1,sharey=False)\n",
    "sns.scatterplot(x = train['GrLivArea'], y = train['SalePrice'],  ax=ax1)\n",
    "sns.regplot(x=train['GrLivArea'], y=train['SalePrice'], ax=ax1); # plot a regression line. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpert the blue strip in the graph above. How well does a linear model fit the data? Does this graph support the homoscedasticity assumtion?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b8506cb7-41ec-494e-8af5-87ce9b55745c",
    "_uuid": "9fac9095-b1a8-4ce1-951e-ef2201ca0f68"
   },
   "source": [
    "Sometimes we may be trying to fit a linear regression model when the data might not be so linear, or the function may need another degree of freedom to fit the data.  \n",
    "In that case, we may need to change our model depending on the data to get the best possible fit.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-17T14:19:51.443210Z",
     "iopub.status.busy": "2021-06-17T14:19:51.442813Z",
     "iopub.status.idle": "2021-06-17T14:19:51.446861Z",
     "shell.execute_reply": "2021-06-17T14:19:51.446034Z",
     "shell.execute_reply.started": "2021-06-17T14:19:51.443180Z"
    }
   },
   "source": [
    "## Homoscedasticity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-17T14:27:30.863822Z",
     "iopub.status.busy": "2021-06-17T14:27:30.863349Z",
     "iopub.status.idle": "2021-06-17T14:27:30.888380Z",
     "shell.execute_reply": "2021-06-17T14:27:30.886281Z",
     "shell.execute_reply.started": "2021-06-17T14:27:30.863789Z"
    }
   },
   "source": [
    "**Homoscedasticity (Constant Variance):** \n",
    "Homoscedasticity describes a situation in which the error/variance/noise in the relationship between the independent variables and the dependent variable is the same across all values of the independent variable.  \n",
    "In other words, there is a constant variance present in the response variable as the predictor variable increases.  \n",
    "If the \"noise\" is not the same across the values of an independent variable, we call that **Heteroscedasticity**.  \n",
    "As you can guess, it is the opposite of **Homoscedasticity.**\n",
    "\n",
    "\n",
    "<p><img src=\"https://www.dummies.com/wp-content/uploads/415147.image1.jpg\" style=\"float:center\"></img></p>\n",
    "\n",
    "This plot above is an excellent example of Homoscedasticity.  \n",
    "\n",
    "As you can see, the residual variance is the same as the value of the predictor variable increases.  \n",
    "One way to fix this Heteroscedasticity is by using a transformation method like log-transformation. We will do that later.\n",
    "\n",
    "In the **GrLivArea**-**SalePrice** scatter plot we see that, as the value of **GrLivArea** \n",
    "increases, the variance also increases (i.e. we have heteroscedasticity)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's look at the target - SalePrice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-17T13:51:56.378839Z",
     "iopub.status.busy": "2021-06-17T13:51:56.378375Z",
     "iopub.status.idle": "2021-06-17T13:51:56.391444Z",
     "shell.execute_reply": "2021-06-17T13:51:56.390261Z",
     "shell.execute_reply.started": "2021-06-17T13:51:56.378799Z"
    }
   },
   "source": [
    "Plot a histogram of the label (=target variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-18T05:34:23.627932Z",
     "iopub.status.busy": "2021-10-18T05:34:23.627620Z",
     "iopub.status.idle": "2021-10-18T05:34:23.631689Z",
     "shell.execute_reply": "2021-10-18T05:34:23.630909Z",
     "shell.execute_reply.started": "2021-10-18T05:34:23.627902Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-17T13:53:14.932605Z",
     "iopub.status.busy": "2021-06-17T13:53:14.932225Z",
     "iopub.status.idle": "2021-06-17T13:53:14.943737Z",
     "shell.execute_reply": "2021-06-17T13:53:14.942497Z",
     "shell.execute_reply.started": "2021-06-17T13:53:14.932578Z"
    }
   },
   "source": [
    "It seems that the distribution is not symmetric and might have some outliers. Another way of visualizing this is by using a boxpot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.boxplot(train.loc[:,'SalePrice'], orient='v');\n",
    "plt.title('Box Plot');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "67c66715-2ba3-4245-9759-1047606e4f12",
    "_uuid": "f1be8cbe-c8cc-4096-a38b-a81c939653f4"
   },
   "source": [
    "These **two** charts above can tell us a lot about our target variable:\n",
    "* Our target variable, **SalePrice** does not seem to be normally distributed.\n",
    "* The target variable is right-skewed. \n",
    "* There are multiple outliers in the variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skewness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skewness is an important attribute of a normal distribtution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "71602011-62bf-436f-b4a5-fe1ef34e2d97",
    "_uuid": "2f508e99-97b3-4522-9569-ab32b8d6c810"
   },
   "source": [
    "<b>Skewness</b> \n",
    "According to Wikipedia, \n",
    "\n",
    "* \"In probability theory and statistics, skewness is a measure of the asymmetry of the probability distribution of a real-valued random variable about its mean. The skewness value can be positive, zero, negative, or undefined.\n",
    "\n",
    "* For a unimodal distribution, negative skew commonly indicates that the tail is on the left side of the distribution, and positive skew indicates that the tail is on the right. In cases where one tail is long but the other tail is fat, skewness does not obey a simple rule. For example, a zero value means that the tails on both sides of the mean balance out overall; this is the case for a symmetric distribution, but can also be true for an asymmetric distribution where one tail is long and thin, and the other is short but fat.\"\n",
    "\n",
    "The following image illustrates the concept of skewness:  \n",
    "![image](https://cdn-images-1.medium.com/max/1600/1*nj-Ch3AUFmkd0JUSOW_bTQ.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find out the skewness of our label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "dbf1c7a3-c1cf-4a59-b7be-08c65fa50fad",
    "_kg_hide-input": true,
    "_uuid": "6198f8cd-82de-49c5-a7c8-35b0bf5df409",
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-17T12:53:30.984220Z",
     "iopub.status.busy": "2021-06-17T12:53:30.983828Z",
     "iopub.status.idle": "2021-06-17T12:53:30.992341Z",
     "shell.execute_reply": "2021-06-17T12:53:30.991186Z",
     "shell.execute_reply.started": "2021-06-17T12:53:30.984191Z"
    }
   },
   "source": [
    "So, now we can see that there is quite a bit of Skewness in the target variable.  \n",
    "This might indicate that some transformation is needed on our target variable. \n",
    "Right Skewness can usually be fixed by using log transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-20T13:34:54.383434Z",
     "iopub.status.busy": "2021-06-20T13:34:54.383059Z",
     "iopub.status.idle": "2021-06-20T13:34:54.392285Z",
     "shell.execute_reply": "2021-06-20T13:34:54.391241Z",
     "shell.execute_reply.started": "2021-06-20T13:34:54.383406Z"
    }
   },
   "source": [
    "### Fixing skewness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's draw the same graphs from above but this time we'll first transform the target variable using a log function \n",
    "\n",
    "Create a new column in your train dataframe **logSalePrice** and set it as the log of the label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the distribution of **logSalePrice**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "8af2e89b-d316-45ae-8053-68bf53b15f87",
    "_kg_hide-input": true,
    "_uuid": "ef5a8648-6d0e-40de-9bd3-d71d9d9cba4a",
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a box-plot of **logSalePrice**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a scatter plot of **GrLivArea** vs **logSalePrice**. Is the heteroscedasticity different the new scatter plot?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the log transformation fixes the heteroscedasticity of the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-20T13:34:54.383434Z",
     "iopub.status.busy": "2021-06-20T13:34:54.383059Z",
     "iopub.status.idle": "2021-06-20T13:34:54.392285Z",
     "shell.execute_reply": "2021-06-20T13:34:54.391241Z",
     "shell.execute_reply.started": "2021-06-20T13:34:54.383406Z"
    }
   },
   "source": [
    "## The target variable is normally distributed when the features values are fixed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In plain words, we want to make sure that for each $x_0$ value of the feature $X$, $Y\\mid_{X=x_0}$ is a random variable following a normal distribution and its mean lies on the regression line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's choose one of the values of 'OverallQual' feature:  \n",
    "Do the following for SalePrice and logSalePrice:\n",
    "1. Plot histogram of the target for OverallQual == 8\n",
    "2. Check the skewness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-30T12:26:26.422749Z",
     "iopub.status.busy": "2021-06-30T12:26:26.422378Z",
     "iopub.status.idle": "2021-06-30T12:26:26.440529Z",
     "shell.execute_reply": "2021-06-30T12:26:26.439206Z",
     "shell.execute_reply.started": "2021-06-30T12:26:26.422722Z"
    }
   },
   "source": [
    "SalePrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-30T12:26:15.647208Z",
     "iopub.status.busy": "2021-06-30T12:26:15.646837Z",
     "iopub.status.idle": "2021-06-30T12:26:15.674588Z",
     "shell.execute_reply": "2021-06-30T12:26:15.673556Z",
     "shell.execute_reply.started": "2021-06-30T12:26:15.647181Z"
    }
   },
   "source": [
    "logSalePrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-30T12:28:18.230057Z",
     "iopub.status.busy": "2021-06-30T12:28:18.229586Z",
     "iopub.status.idle": "2021-06-30T12:28:18.241713Z",
     "shell.execute_reply": "2021-06-30T12:28:18.240583Z",
     "shell.execute_reply.started": "2021-06-30T12:28:18.230024Z"
    }
   },
   "source": [
    "So, as we can see the log transforamtion makes the target variable follow our assumption more closely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select additional features and values and check the distibution of the label and its log-transformation given those values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No or Little multicollinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7b8eb81b-7d4a-4461-84ba-a84dacbf060b",
    "_uuid": "0b5fc8ff-4aa3-44b3-95e2-b8b6f7ed3e47"
   },
   "source": [
    "Multicollinearity is when there is a strong correlation between independent variables. Linear regression or multilinear regression requires independent variables to have few or no similar features. Multicollinearity can lead to a variety of problems, including:\n",
    "* The effect of predictor variables estimated by our regression will depend on what other variables are included in our model. \n",
    "* Predictors can have wildly different results depending on the observations in our sample, and small changes in samples can result in very different estimated effects. \n",
    "* We can no longer interpret a coefficient on a variable as the effect on the target of a one-unit increase in that variable holding the other variables constant. The reason behind that is, when predictors are strongly correlated, there is no scenario in which one variable can change without a conditional change in another variable.\n",
    "\n",
    "A very common way to visualize correlations is using a heat map. An even better way is a clustermap which also allows you to see whether there is multicollinearity or not. We'll discuss methods for solving multicollinearity (such as ridge and lasso methods) later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-08-19T02:14:30.769164Z",
     "iopub.status.idle": "2021-08-19T02:14:30.769458Z",
     "shell.execute_reply": "2021-08-19T02:14:30.769314Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30, 30))\n",
    "sns.clustermap(train.drop(columns = ['SalePrice']).corr(), xticklabels=True, yticklabels=True)\n",
    "plt.title(\"ClusterMap of all the Features\", fontsize = 30)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5d747552-2932-484a-99f7-41089287aef8",
    "_uuid": "f6dd55c1-7366-4828-9be5-c68ea99cf070"
   },
   "source": [
    "The clustermap arranges correlated features side-by-side. This way it is easier to identify correlated clusters of features (bright squares).  \n",
    "As we can see, the multicollinearity still exists between various features (bright squares of 2 features or more). However, we will keep them for now for the sake of learning and let the models do the clean up later on. Let's go through some of the correlations. \n",
    "\n",
    "* **GarageYrBlt** and **YearBuilt**. \n",
    "* **TotRmsAbvGrd** and **GrLivArea**. \n",
    "* **GarageCars** and **GarageArea**. \n",
    "* **1stFlrSF** and **TotalBsmtSF**  \n",
    "\n",
    "If we were using only multiple linear regression, it would have been better to delete one feature of each of these feature-pairs from the dataset to fit better multiple linear regression models.  \n",
    "However since scikit learn modules makes it easy to implement them and get the best possible outcome, we will be using many algorithms.\n",
    "Therefore, we will keep all the features for now. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-20T13:52:28.855242Z",
     "iopub.status.busy": "2021-06-20T13:52:28.854855Z",
     "iopub.status.idle": "2021-06-20T13:52:28.862653Z",
     "shell.execute_reply": "2021-06-20T13:52:28.861416Z",
     "shell.execute_reply.started": "2021-06-20T13:52:28.855213Z"
    }
   },
   "source": [
    "# Preparing data for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop the **Id** column from both the train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the targets into separate series called **y_train** and **y_test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6549f4c5-9d55-4b8f-93e9-33c3cb8add41",
    "_kg_hide-input": true,
    "_uuid": "f3c6da13-9a5d-40b6-b2a9-b66e5e729bc2",
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining train and test datasets together\n",
    "In this section we'll fill in missing values, and do some feature engineering and selection. Since these are operations we can apply to both the training and test set, will concatenate them so that we can do all the work at once.\n",
    "\n",
    "We just need to add a column which indicates which rows came from which dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-08-19T02:14:30.770236Z",
     "iopub.status.idle": "2021-08-19T02:14:30.770531Z",
     "shell.execute_reply": "2021-08-19T02:14:30.770390Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train['set'] = 'Train'\n",
    "X_test['set'] = 'Test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-08-19T02:14:30.771366Z",
     "iopub.status.idle": "2021-08-19T02:14:30.771698Z",
     "shell.execute_reply": "2021-08-19T02:14:30.771522Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_data = pd.concat([X_train, X_test]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop the target variable from all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-17T14:43:23.440544Z",
     "iopub.status.busy": "2021-06-17T14:43:23.440150Z",
     "iopub.status.idle": "2021-06-17T14:43:23.444247Z",
     "shell.execute_reply": "2021-06-17T14:43:23.443432Z",
     "shell.execute_reply.started": "2021-06-17T14:43:23.440515Z"
    }
   },
   "source": [
    "## Missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function that, given a dataframe df, creates a table with the number and percentage of missing values in each column, sorted in desceding order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6ad3c174-65b0-4094-a239-566e109c02f9",
    "_kg_hide-input": true,
    "_uuid": "4f0f52cc-764e-420e-9737-b5cf6a2964b9",
    "execution": {
     "iopub.status.busy": "2021-08-19T02:14:30.772592Z",
     "iopub.status.idle": "2021-08-19T02:14:30.772898Z",
     "shell.execute_reply": "2021-08-19T02:14:30.772748Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def missing_percentage(df):\n",
    "    #write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does your function output on all_data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b4c40d68-6fe3-48a3-83a9-d3c8c2ff9409",
    "_uuid": "bed4ad35-8ef0-4048-8212-d7630b73c841"
   },
   "source": [
    "**Imputing Missing Values**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some missing values are intentionally left blank, for example: In the Alley feature \n",
    "there are blank values meaning that there are no alley's in that specific house. \n",
    "\n",
    "Here is a list of all features for which a missing value should remain blank:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-08-19T02:14:30.774086Z",
     "iopub.status.idle": "2021-08-19T02:14:30.774388Z",
     "shell.execute_reply": "2021-08-19T02:14:30.774242Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "missing_val_col_true_none = [\"Alley\", \"PoolQC\", \"MiscFeature\", \"Fence\",\n",
    "                             \"FireplaceQu\", \"GarageType\", \"GarageFinish\", \n",
    "                             \"GarageQual\", \"GarageCond\", 'BsmtQual', \n",
    "                             'BsmtCond','BsmtExposure', 'BsmtFinType1', \n",
    "                             'BsmtFinType2', 'MasVnrType']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace all missing values with np.nan. Note that some missing values may be encoded with different value (e.g. None or 'None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "496b6ff7-801e-4191-9ea9-d325f53e5e38",
    "_kg_hide-input": true,
    "_uuid": "06234ae2-b23d-4c11-9de3-5a0d89f63780",
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following features the null values indicate that the feature doesn't exist in the house, and therefore should be replaced with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-08-19T02:14:30.775211Z",
     "iopub.status.idle": "2021-08-19T02:14:30.775507Z",
     "shell.execute_reply": "2021-08-19T02:14:30.775364Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "missing_val_col_zero = ['BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF',\n",
    "                        'TotalBsmtSF', 'BsmtFullBath',  'BsmtHalfBath', \n",
    "                        'GarageArea', 'GarageCars', 'MasVnrArea']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace missing values in the above columns by 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "9223ac79-96ea-4c6b-a821-31e2ee3e6928",
    "_kg_hide-input": true,
    "_uuid": "70e98943-19bf-4965-8818-6fcce53c370c",
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some missing values are actually missing. It is sometimes reasonable to fill them with the mean value of the feature (across all entries). This way, the mean the of feature is preserved.\n",
    "We will use this method on the LotFrontage feature, however, instead of simply filling the missing values with the mean of the feature across the entire dataset, we will calculate the mean for every neighborhood independently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-20T10:04:45.226978Z",
     "iopub.status.busy": "2021-07-20T10:04:45.226701Z",
     "iopub.status.idle": "2021-07-20T10:04:45.235033Z",
     "shell.execute_reply": "2021-07-20T10:04:45.234213Z",
     "shell.execute_reply.started": "2021-07-20T10:04:45.226952Z"
    },
    "tags": []
   },
   "source": [
    "Create a series, with neighborhood names as indices, containing the mean LotFrontage value **in X_train** of each neighborhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace all missing values in all_data['LotFrontage'] by the mean LotFrontage in their neighborhood, according to X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Why did we only use the X_train to calculate the mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another example of such a feature is GarageYrBuilt. Repeat the process above for this feature, and take the neighborhood median for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Missing values in categorical features**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some categorical features in the data are encoded as integers, even though they should be strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-08-19T02:14:30.776308Z",
     "iopub.status.idle": "2021-08-19T02:14:30.776604Z",
     "shell.execute_reply": "2021-08-19T02:14:30.776460Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_data[['MSSubClass', 'YrSold', 'MoSold']].dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change these to string dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-08-19T02:14:30.777394Z",
     "iopub.status.idle": "2021-08-19T02:14:30.777688Z",
     "shell.execute_reply": "2021-08-19T02:14:30.777545Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_data[['MSSubClass', 'YrSold', 'MoSold']].dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since in categorical features there is no concept of \"mean\", we will fill the missing values with the most common category. Replace missing values in the following features by their most common value (i.e. their <a href= \"https://en.wikipedia.org/wiki/Mode_(statistics)#:~:text=The%20mode%20is%20the%20value,most%20likely%20to%20be%20sampled.\">mode</a>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-08-19T02:14:30.778505Z",
     "iopub.status.idle": "2021-08-19T02:14:30.778797Z",
     "shell.execute_reply": "2021-08-19T02:14:30.778656Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "['SaleType','Functional','MSZoning','Utilities',\n",
    " 'Exterior1st','Exterior2nd','KitchenQual',\n",
    " 'SaleType','Electrical']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check how many missing values are left undealt with. Notice that we are checking missing values only on features which we didn't fill with NaN on purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, there are no missing value left. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating New Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create new features called ['haspool','has2ndfloor','hasgarage','hasbsmt','hasfireplace'] which attain the value 1 if the house has the corresponding amenity (pool, 2nd floor etc) and 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deleting features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing the features used to create the new features so avoid feature dependancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-17T14:42:20.284437Z",
     "iopub.status.busy": "2021-06-17T14:42:20.284001Z",
     "iopub.status.idle": "2021-06-17T14:42:20.300074Z",
     "shell.execute_reply": "2021-06-17T14:42:20.298912Z",
     "shell.execute_reply.started": "2021-06-17T14:42:20.284407Z"
    }
   },
   "source": [
    "Use value_counts() to check which features have low variability. Remove the 3 features with the lowest variability (take care not to remove the columns 'set', though)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: why have we decided to delete these features?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: value_counts shows extremely low variablity of values. Therefore these features will probably not contribute to our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "986e1dd1-d8a9-45a2-bd30-38247b0d8cf3",
    "_uuid": "1986bfdc-783f-4c72-b4d6-1edfcb11d540"
   },
   "source": [
    "## Creating Dummy Variables. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-20T10:01:17.746448Z",
     "iopub.status.busy": "2021-07-20T10:01:17.746055Z",
     "iopub.status.idle": "2021-07-20T10:01:17.756264Z",
     "shell.execute_reply": "2021-07-20T10:01:17.754965Z",
     "shell.execute_reply.started": "2021-07-20T10:01:17.746418Z"
    }
   },
   "source": [
    "A dummy variable is a way to convert categorical variable into numerical variable used in regression to represent subgroups of the feature.  \n",
    "For example: \n",
    "if we have a feature with 'yes' / 'no' categories, we would use a 0,1 dummy variable where a 1 is given for 'yes' and 0 for 'no'.  \n",
    "\n",
    "Dummy variables are useful because they enable us to use a single regression equation to represent multiple groups.  \n",
    "This means that we dont need to write out separate models for each subgroup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find all categorical features and use pd.get_dummies to replace them with dummy variable. Make sure to remove the original features from your dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "91f15d8f-2cb5-4c14-969b-f63305325b91",
    "_kg_hide-input": true,
    "_uuid": "178c82cc-928b-4e2b-ba13-e6a3f79afc89",
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spltting back to train and test\n",
    "Split the all_data dataframe back to X_train and X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start training models and comparing them, we need some notion of \"quality\" of a model, that will allow us to compare various models and choose the best one.\n",
    "\n",
    "A most common such notion is the **Mean Squared Error (MSE)** which you have encountered in class. For a given label vector $y$ and predicted labels $\\hat{y}$, we have $MSE(y, \\hat{y})=\\frac{1}{n}\\sum_{i=1}^{n}{(y_i - \\hat{y_i}) ^ 2}$.\n",
    "First, let us consider a constant model - that is, a model that predicts the same value for all samples, regardless of their features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: What is the constant model with the least MSE? Prove your answer. What is its MSE on our train and test set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the MSE of your model against the constant model you found. Use sklearn.metrics.mean_squared_error. What is the model's score for the test set? What is its score for the training set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-08-19T02:14:30.779796Z",
     "iopub.status.idle": "2021-08-19T02:14:30.780097Z",
     "shell.execute_reply": "2021-08-19T02:14:30.779952Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's train a model using a single feature, GrLivArea, how much better is it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-08-19T02:14:30.780874Z",
     "iopub.status.idle": "2021-08-19T02:14:30.781163Z",
     "shell.execute_reply": "2021-08-19T02:14:30.781022Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "initiate an instance of sklearn.linear_model.LinearRegression and use it to fit a model with 'GrLivArea' as its only features. What is its MSE on the train and test set? What is the <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression.score\">model's score</a> on these sets?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OK, now lets try using all features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit a linear regression model on all fetures using the same package    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is its MSE on the test and train set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-27T15:12:00.135257Z",
     "iopub.status.busy": "2021-06-27T15:12:00.134835Z",
     "iopub.status.idle": "2021-06-27T15:12:00.139807Z",
     "shell.execute_reply": "2021-06-27T15:12:00.138570Z",
     "shell.execute_reply.started": "2021-06-27T15:12:00.135227Z"
    }
   },
   "source": [
    "Cool! So adding more features and making our model more complex improves our model. Will more complex models always perform better than simpler models? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-27T15:12:00.135257Z",
     "iopub.status.busy": "2021-06-27T15:12:00.134835Z",
     "iopub.status.idle": "2021-06-27T15:12:00.139807Z",
     "shell.execute_reply": "2021-06-27T15:12:00.138570Z",
     "shell.execute_reply.started": "2021-06-27T15:12:00.135227Z"
    }
   },
   "source": [
    "## Let's try adding degree-2 polynomial features\n",
    "that is, the square of every feature and the product of every pair of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use <a href=https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html>sklearn.preprocessing.PolynomialFeatures</a> to create a dataframe of all degree 2 combinations of features in your data, and apply a linear regression to the transformed dataset.\n",
    "\n",
    "_Note:_ use <a href= \"https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html\">sklearn.pipeline.Pipeline</a> to create a model that squares all features and predicts in one go. This will also be useful for future steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-08-19T02:14:30.782028Z",
     "iopub.status.idle": "2021-08-19T02:14:30.782325Z",
     "shell.execute_reply": "2021-08-19T02:14:30.782180Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is your new model's MSE on the training and test sets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh dear! The train MSE is now almost 0, but the test score is even worse than that of the constant model. This is since the model is now over complicated and rather than learning the underlying structure of the data, it is memorizing the training data. This is called overfitting. One common way to combat this is by adding a regularization term, which penalizes models for overcomplicating and motivates them to choose simpler solution. \n",
    "\n",
    "Let's try adding an L1 regularization term (this is called the LASSO model), which adds the sum of absolute values of the weights to the loss. Use <a href= \"https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html\">sklearn.linear_model.Lasso</a> with an alpha value of 0.009 and see if the model improves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-08-19T02:14:30.783158Z",
     "iopub.status.idle": "2021-08-19T02:14:30.783455Z",
     "shell.execute_reply": "2021-08-19T02:14:30.783311Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "lasso = Lasso(0.009)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a little better but still not as good as it should be. Let's try normalizing the data and see if that helps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the <a href= \"https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html?highlight=standard%20scaler#sklearn.preprocessing.StandardScaler\"> sklearn.preprocessing.StandardScaler</a> to normalize your data to have mean 0 and std 1, and fit your model with the normalized data, after all steps above (lasso and deg-2 poly transformation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-08-19T02:14:30.784226Z",
     "iopub.status.idle": "2021-08-19T02:14:30.784520Z",
     "shell.execute_reply": "2021-08-19T02:14:30.784377Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the MSE of your model on the training and test sets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good, we fixed the overfitting and even made a slight improvement.\n",
    "Why is normalization important when adding regularization? What would happen if we normalize in unregularized linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A note about regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the unregularized setting, normalizing doesn't affect the solution. Consider $X, y$ and $A, b$ which minimizes $||AX+b - y||$. Now suppose we normalize $X$ to get $D(X-\\bar{X})$ where $D$ is a diagonal matrix that scales $X$ - we now try to minimize $||AD(X-\\bar{X})+b - y||$, but rewriting $A'=AD, b'=b - AD \\bar{X}$ we see that we can move the normalization into the solution that minimizes with resepect to the unnormalized $X$. However, when adding regularization, we \"punish\" each feature based on the magnitude of its coefficient, so scaling a feature by $c$ and its coefficient by $\\frac{1}{c}$  are no longer equivalent since we change the punishment for that coefficient. This means that we disproportionately punish features of smaller scale and the solution may become dominated by features of larger scale which require smaller coefficients to produce the same impact on the prediction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
